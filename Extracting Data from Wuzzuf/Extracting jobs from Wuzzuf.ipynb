{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ce2fce-5dc3-4a31-8a60-18ec688e136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a2c8dd7-c9a0-4ffa-a312-85dae7e1f6e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Page switch\n",
      "Pages Ended \n",
      "Total pages:  117\n"
     ]
    }
   ],
   "source": [
    "job_titles = []\n",
    "job_skills = []\n",
    "locations = []\n",
    "companies = []\n",
    "links = []\n",
    "date = []\n",
    "\n",
    "page_num = 0\n",
    "while True:\n",
    "    URl = f\"https://wuzzuf.net/search/jobs?q=Data%20Analysts&start={page_num}&a=navbl\"\n",
    "    response = requests.get(URl) # send HTTP get request \n",
    "    source = response.content\n",
    "    \n",
    "    # use BeautifulSoup to reads and extracts data from the HTML\n",
    "    soup = BeautifulSoup(source,\"lxml\")\n",
    "\n",
    "    # Number of pages for loop\n",
    "    page_limit = int(soup.find(\"strong\").text.replace(\",\",\"\"))\n",
    "    if (page_limit // 15) < page_num:\n",
    "        print(\"Pages Ended \")\n",
    "        print(\"Total pages: \",page_num)\n",
    "        break\n",
    "    \n",
    "    # Extract Job titles and job link\n",
    "    job_title = soup.find_all(\"h2\",{\"class\":\"css-193uk2c\" })\n",
    "    \n",
    "    # Extract Company name\n",
    "    company = soup.find_all(\"a\",{\"class\":\"css-ipsyv7\"})\n",
    "    \n",
    "    # Extract Locations\n",
    "    location = soup.find_all(\"span\",{\"class\":\"css-16x61xq\"})\n",
    "    \n",
    "    # Extract job skills\n",
    "    job_skill = soup.find_all(\"div\",{\"class\":\"css-1rhj4yg\"})\n",
    "    \n",
    "    # Extract Publication date\n",
    "    posted_new = soup.find_all(\"div\",{\"class\":\"css-eg55jf\"})\n",
    "    posted_old = soup.find_all(\"div\",{\"class\":\"css-1jldrig\"})\n",
    "    posted = [*posted_new,*posted_old]\n",
    "    \n",
    "    for i in range(len(job_title)):\n",
    "        job_titles.append(job_title[i].text)\n",
    "        links.append(job_title[i].find(\"a\").attrs['href'])\n",
    "        job_skills.append(job_skill[i].text)\n",
    "        companies.append(company[i].text)\n",
    "        locations.append(location[i].text)\n",
    "        date.append(posted[i].text)\n",
    "        \n",
    "    page_num += 1\n",
    "    print(\"Page switch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8b4820-235b-48b9-a577-39405fc606ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Extract Salary from each job\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "salary_list = []\n",
    "\n",
    "for link in links:\n",
    "    url = f\"https://wuzzuf.net{link}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    salary_tag = soup.find(\"span\", class_=\"css-iu2m7n\")\n",
    "    if salary_tag:\n",
    "        salary_text = salary_tag.text.strip()\n",
    "\n",
    "        if \"Confidential\" in salary_text:\n",
    "            salary_list.append(\"Confidential\")\n",
    "        else:\n",
    "            salary_list.append(salary_text)\n",
    "\n",
    "    else:\n",
    "        salary_list.append(\"Not Mentioned\")\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40d33685-96a1-439e-9412-1690e42c58b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Extract requirements from each job\n",
    "requirements = []\n",
    "\n",
    "for link in links:\n",
    "    url = f\"https://wuzzuf.net{link}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "\n",
    "    # Find the Job Requirements section by heading text\n",
    "    section = soup.find(lambda tag: tag.name in [\"h2\", \"h3\"] and tag.get_text(strip=True).lower() in [\"job requirements\",\"requirements\"])\n",
    "    if section:\n",
    "        container = section.find_next([\"div\", \"ul\"])\n",
    "        if container:\n",
    "            items = container.find_all([\"li\", \"p\"])\n",
    "            job_requirements = [\n",
    "                item.get_text(strip=True)\n",
    "                for item in items\n",
    "                if item.get_text(strip=True)\n",
    "            ]\n",
    "            requirements.append(\" | \".join(job_requirements) if job_requirements else None)\n",
    "        else:\n",
    "            requirements.append(None)\n",
    "    else:\n",
    "        requirements.append(None)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcb890-2b6d-451b-9848-e9b66098a622",
   "metadata": {},
   "source": [
    "## Store data in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61b690b7-6eb8-4a41-ae9c-7eb8ad99aa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printed successful\n"
     ]
    }
   ],
   "source": [
    "# Create csv file to store data\n",
    "\n",
    "file_list = [date,job_titles, companies, locations, job_skills, links,salary_list,requirements]\n",
    "afterlongest= zip_longest(*file_list)\n",
    "\n",
    "with open(r\"C:\\Users\\HP\\Desktop\\Data Analysts Jobs.csv\", \"w\", newline=\"\", encoding=\"utf-8-sig\") as myfile :\n",
    "    wr = csv.writer(myfile)\n",
    "    wr.writerow([\"Posted date\",\"Job title\",\"Company name\",\"Location\",\"Skills\",\"Links\",\"Salaries\",\"Requirements\"])\n",
    "    wr.writerows(afterlongest)\n",
    "    print(\"Printed successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d278d28d-de5a-4146-b970-db10dc22192c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Posted date</th>\n",
       "      <th>Job title</th>\n",
       "      <th>Company name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Links</th>\n",
       "      <th>Salaries</th>\n",
       "      <th>Requirements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 days ago</td>\n",
       "      <td>Junior Data Analyst</td>\n",
       "      <td>Wadi Group -</td>\n",
       "      <td>Sheikh Zayed, Giza, Egypt</td>\n",
       "      <td>Full TimeOn-siteEntry Level · 0 - 2 Yrs of Exp...</td>\n",
       "      <td>/jobs/p/deipudnut5wh-junior-data-analyst-wadi-...</td>\n",
       "      <td>Not Mentioned</td>\n",
       "      <td>Ready to Embark on Your Data Analysis Journey?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Data Analyst manager</td>\n",
       "      <td>Confidential -</td>\n",
       "      <td>New Cairo, Cairo, Egypt</td>\n",
       "      <td>Full TimeOn-siteManager · 5 - 7 Yrs of Exp · m...</td>\n",
       "      <td>/jobs/p/wm0gnesr8hq3-data-analyst-manager-cair...</td>\n",
       "      <td>Not Mentioned</td>\n",
       "      <td>Bachelor’s degree in Data Science, Statistics,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 days ago</td>\n",
       "      <td>Commercial Specialist-Data Analyst (Chinese Sp...</td>\n",
       "      <td>OPPO Egypt  -</td>\n",
       "      <td>Nasr City, Cairo, Egypt</td>\n",
       "      <td>Full TimeOn-siteEntry Level · 0 - 2 Yrs of Exp...</td>\n",
       "      <td>/jobs/p/elmap2ottk20-commercial-specialist-dat...</td>\n",
       "      <td>Not Mentioned</td>\n",
       "      <td>1-2 years of exp. Fresh graduates are most wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 days ago</td>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Al Ahly capital holding - Al Ahly Tamkeen -</td>\n",
       "      <td>Zamalek, Cairo, Egypt</td>\n",
       "      <td>Full TimeOn-siteExperienced · 5 - 7 Yrs of Exp...</td>\n",
       "      <td>/jobs/p/pdbhnhmsyty2-senior-data-analyst-al-ah...</td>\n",
       "      <td>Not Mentioned</td>\n",
       "      <td>Bachelor's or master's degree in a quantitativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 days ago</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>HIGHBASE TRADING W.L.L -</td>\n",
       "      <td>Cairo, Egypt</td>\n",
       "      <td>Full TimeRemoteExperienced · 4 - 6 Yrs of Exp ...</td>\n",
       "      <td>/jobs/p/l2cctgjsulvx-data-analyst-highbase-tra...</td>\n",
       "      <td>Not Mentioned</td>\n",
       "      <td>Required Skills &amp; Experience: | Proven experie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Posted date                                          Job title  \\\n",
       "0   3 days ago                                Junior Data Analyst   \n",
       "1   4 days ago                               Data Analyst manager   \n",
       "2   4 days ago  Commercial Specialist-Data Analyst (Chinese Sp...   \n",
       "3  12 days ago                                Senior Data Analyst   \n",
       "4  12 days ago                                       Data Analyst   \n",
       "\n",
       "                                  Company name                    Location  \\\n",
       "0                                 Wadi Group -  Sheikh Zayed, Giza, Egypt    \n",
       "1                               Confidential -    New Cairo, Cairo, Egypt    \n",
       "2                                OPPO Egypt  -    Nasr City, Cairo, Egypt    \n",
       "3  Al Ahly capital holding - Al Ahly Tamkeen -      Zamalek, Cairo, Egypt    \n",
       "4                     HIGHBASE TRADING W.L.L -               Cairo, Egypt    \n",
       "\n",
       "                                              Skills  \\\n",
       "0  Full TimeOn-siteEntry Level · 0 - 2 Yrs of Exp...   \n",
       "1  Full TimeOn-siteManager · 5 - 7 Yrs of Exp · m...   \n",
       "2  Full TimeOn-siteEntry Level · 0 - 2 Yrs of Exp...   \n",
       "3  Full TimeOn-siteExperienced · 5 - 7 Yrs of Exp...   \n",
       "4  Full TimeRemoteExperienced · 4 - 6 Yrs of Exp ...   \n",
       "\n",
       "                                               Links       Salaries  \\\n",
       "0  /jobs/p/deipudnut5wh-junior-data-analyst-wadi-...  Not Mentioned   \n",
       "1  /jobs/p/wm0gnesr8hq3-data-analyst-manager-cair...  Not Mentioned   \n",
       "2  /jobs/p/elmap2ottk20-commercial-specialist-dat...  Not Mentioned   \n",
       "3  /jobs/p/pdbhnhmsyty2-senior-data-analyst-al-ah...  Not Mentioned   \n",
       "4  /jobs/p/l2cctgjsulvx-data-analyst-highbase-tra...  Not Mentioned   \n",
       "\n",
       "                                        Requirements  \n",
       "0  Ready to Embark on Your Data Analysis Journey?...  \n",
       "1  Bachelor’s degree in Data Science, Statistics,...  \n",
       "2  1-2 years of exp. Fresh graduates are most wel...  \n",
       "3  Bachelor's or master's degree in a quantitativ...  \n",
       "4  Required Skills & Experience: | Proven experie...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\Data Analysts Jobs.csv\")\n",
    "data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
